{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b70e3de3",
   "metadata": {},
   "source": [
    "Ulysses Curiosity offers a group of 10 preconfigured probing tasks for PT-br language, following the original paper [(Conneau et al. 2018)](https://aclanthology.org/P18-1198/). This notebook showcases how to instantiate a preconfigured probing task with a HuggingFace Transformers and a vanilla PyTorch Bidirectional LSTM model.\n",
    "\n",
    "\n",
    "**Table of Contents**\n",
    "1. [HuggingFace Transformers example](#HuggingFace-Transformers-example)\n",
    "2. [Vanilla PyTorch example](#Vanilla-PyTorch-example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce31a5ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:56:08.972160Z",
     "start_time": "2022-10-17T13:56:08.126955Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/nvme/ulysses-curiosity/venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# (1): import needed packages.\n",
    "import typing as t\n",
    "import curiosidade\n",
    "import torch\n",
    "import torch.nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbef985a",
   "metadata": {},
   "source": [
    "# HuggingFace Transformers example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b2cc61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:56:09.293780Z",
     "start_time": "2022-10-17T13:56:08.973867Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74fcbcf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:56:12.785956Z",
     "start_time": "2022-10-17T13:56:09.295271Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at neuralmind/bert-base-portuguese-cased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'DistilBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# (2): load your pretrained model.\n",
    "model_name = \"neuralmind/bert-base-portuguese-cased\"\n",
    "bert = transformers.BertForTokenClassification.from_pretrained(model_name)\n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7849433",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:56:12.790741Z",
     "start_time": "2022-10-17T13:56:12.787952Z"
    }
   },
   "outputs": [],
   "source": [
    "# (3): set up your probing model.\n",
    "ProbingModel = curiosidade.probers.utils.get_probing_model_for_sequences(hidden_layer_dims=[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2c0d58b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:56:15.121001Z",
     "start_time": "2022-10-17T13:56:12.792346Z"
    }
   },
   "outputs": [],
   "source": [
    "# (4): set up a preconfigured probing task.\n",
    "def fn_text_to_tensor_for_huggingface_transformers(\n",
    "    content: list[str],\n",
    "    labels: list[int],\n",
    "    split: t.Literal[\"train\", \"eval\", \"test\"],\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    n = 2500 # Subsampling to speed up this example.\n",
    "    content = content[:n]  \n",
    "    labels = labels[:n]\n",
    "    \n",
    "    X = tokenizer(\n",
    "        content,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=48,\n",
    "    )\n",
    "    X[\"labels\"] = labels\n",
    "\n",
    "    X = datasets.Dataset.from_dict(X)\n",
    "    X.set_format(\"torch\")\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def accuracy_fn(logits, target):\n",
    "    _, cls_ids = logits.max(axis=-1)\n",
    "    return {\"accuracy\": (cls_ids == target).float().mean().item()}\n",
    "\n",
    "\n",
    "task = curiosidade.ProbingTaskSentenceLength(\n",
    "    fn_text_to_tensor=fn_text_to_tensor_for_huggingface_transformers,\n",
    "    metrics_fn=accuracy_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db94e5bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:56:18.345783Z",
     "start_time": "2022-10-17T13:56:15.122460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prober_container = ProbingModelContainer:\n",
      "(a): Base model: InferencePrunerExtensor(HuggingfaceAdapter(BertForTokenClassification(\n",
      " |  |  (bert): BertModel(\n",
      " |  |    (embeddings): BertEmbeddings(\n",
      " |  |      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
      " |  |      (position_embeddings): Embedding(512, 768)\n",
      " |  |      (token_type_embeddings): Embedding(2, 768)\n",
      " |  |      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |      (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |    )\n",
      " |  |    (encoder): BertEncoder(\n",
      " |  |      (layer): ModuleList(\n",
      " |  |        (0): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (1): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (2): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (3): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (4): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (5): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (6): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (7): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (8): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (9): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (10): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |        (11): BertLayer(\n",
      " |  |          (attention): BertAttention(\n",
      " |  |            (self): BertSelfAttention(\n",
      " |  |              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |            (output): BertSelfOutput(\n",
      " |  |              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      " |  |              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |              (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |            )\n",
      " |  |          )\n",
      " |  |          (intermediate): BertIntermediate(\n",
      " |  |            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      " |  |            (intermediate_act_fn): GELUActivation()\n",
      " |  |          )\n",
      " |  |          (output): BertOutput(\n",
      " |  |            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      " |  |            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      " |  |            (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |          )\n",
      " |  |        )\n",
      " |  |      )\n",
      " |  |    )\n",
      " |  |  )\n",
      " |  |  (dropout): Dropout(p=0.1, inplace=False)\n",
      " |  |  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      " |  |)))\n",
      " | (a): Pruned module(s) (189 in total):\n",
      " |   (0): bert.encoder.layer.1\n",
      " |   (1): bert.encoder.layer.1.attention\n",
      " |   (2): bert.encoder.layer.1.attention.self\n",
      " |   (3): bert.encoder.layer.1.attention.self.query\n",
      " |   (4): bert.encoder.layer.1.attention.self.key\n",
      " |   (5): bert.encoder.layer.1.attention.self.value\n",
      " |   (6): bert.encoder.layer.1.attention.self.dropout\n",
      " |   (7): bert.encoder.layer.1.attention.output\n",
      " |   (8): bert.encoder.layer.1.attention.output.dense\n",
      " |   (9): bert.encoder.layer.1.attention.output.dropout\n",
      " |   (10): bert.encoder.layer.1.attention.output.LayerNorm\n",
      " |   (11): bert.encoder.layer.1.intermediate\n",
      " |   (12): bert.encoder.layer.1.intermediate.dense\n",
      " |   (13): bert.encoder.layer.1.output\n",
      " |   (14): bert.encoder.layer.1.output.dense\n",
      " |   (15): bert.encoder.layer.1.output.dropout\n",
      " |   (16): bert.encoder.layer.1.output.LayerNorm\n",
      " |   (17): bert.encoder.layer.2\n",
      " |   (18): bert.encoder.layer.2.attention\n",
      " |   (19): bert.encoder.layer.2.attention.self\n",
      " |   (20): bert.encoder.layer.2.attention.self.query\n",
      " |   (21): bert.encoder.layer.2.attention.self.key\n",
      " |   (22): bert.encoder.layer.2.attention.self.value\n",
      " |   (23): bert.encoder.layer.2.attention.self.dropout\n",
      " |   (24): bert.encoder.layer.2.attention.output\n",
      " |   (25): bert.encoder.layer.2.attention.output.dense\n",
      " |   (26): bert.encoder.layer.2.attention.output.dropout\n",
      " |   (27): bert.encoder.layer.2.attention.output.LayerNorm\n",
      " |   (28): bert.encoder.layer.2.intermediate\n",
      " |   (29): bert.encoder.layer.2.intermediate.dense\n",
      " |   (30): bert.encoder.layer.2.output\n",
      " |   (31): bert.encoder.layer.2.output.dense\n",
      " |   (32): bert.encoder.layer.2.output.dropout\n",
      " |   (33): bert.encoder.layer.2.output.LayerNorm\n",
      " |   (34): bert.encoder.layer.3\n",
      " |   (35): bert.encoder.layer.3.attention\n",
      " |   (36): bert.encoder.layer.3.attention.self\n",
      " |   (37): bert.encoder.layer.3.attention.self.query\n",
      " |   (38): bert.encoder.layer.3.attention.self.key\n",
      " |   (39): bert.encoder.layer.3.attention.self.value\n",
      " |   (40): bert.encoder.layer.3.attention.self.dropout\n",
      " |   (41): bert.encoder.layer.3.attention.output\n",
      " |   (42): bert.encoder.layer.3.attention.output.dense\n",
      " |   (43): bert.encoder.layer.3.attention.output.dropout\n",
      " |   (44): bert.encoder.layer.3.attention.output.LayerNorm\n",
      " |   (45): bert.encoder.layer.3.intermediate\n",
      " |   (46): bert.encoder.layer.3.intermediate.dense\n",
      " |   (47): bert.encoder.layer.3.output\n",
      " |   (48): bert.encoder.layer.3.output.dense\n",
      " |   (49): bert.encoder.layer.3.output.dropout\n",
      " |   (50): bert.encoder.layer.3.output.LayerNorm\n",
      " |   (51): bert.encoder.layer.4\n",
      " |   (52): bert.encoder.layer.4.attention\n",
      " |   (53): bert.encoder.layer.4.attention.self\n",
      " |   (54): bert.encoder.layer.4.attention.self.query\n",
      " |   (55): bert.encoder.layer.4.attention.self.key\n",
      " |   (56): bert.encoder.layer.4.attention.self.value\n",
      " |   (57): bert.encoder.layer.4.attention.self.dropout\n",
      " |   (58): bert.encoder.layer.4.attention.output\n",
      " |   (59): bert.encoder.layer.4.attention.output.dense\n",
      " |   (60): bert.encoder.layer.4.attention.output.dropout\n",
      " |   (61): bert.encoder.layer.4.attention.output.LayerNorm\n",
      " |   (62): bert.encoder.layer.4.intermediate\n",
      " |   (63): bert.encoder.layer.4.intermediate.dense\n",
      " |   (64): bert.encoder.layer.4.output\n",
      " |   (65): bert.encoder.layer.4.output.dense\n",
      " |   (66): bert.encoder.layer.4.output.dropout\n",
      " |   (67): bert.encoder.layer.4.output.LayerNorm\n",
      " |   (68): bert.encoder.layer.5\n",
      " |   (69): bert.encoder.layer.5.attention\n",
      " |   (70): bert.encoder.layer.5.attention.self\n",
      " |   (71): bert.encoder.layer.5.attention.self.query\n",
      " |   (72): bert.encoder.layer.5.attention.self.key\n",
      " |   (73): bert.encoder.layer.5.attention.self.value\n",
      " |   (74): bert.encoder.layer.5.attention.self.dropout\n",
      " |   (75): bert.encoder.layer.5.attention.output\n",
      " |   (76): bert.encoder.layer.5.attention.output.dense\n",
      " |   (77): bert.encoder.layer.5.attention.output.dropout\n",
      " |   (78): bert.encoder.layer.5.attention.output.LayerNorm\n",
      " |   (79): bert.encoder.layer.5.intermediate\n",
      " |   (80): bert.encoder.layer.5.intermediate.dense\n",
      " |   (81): bert.encoder.layer.5.output\n",
      " |   (82): bert.encoder.layer.5.output.dense\n",
      " |   (83): bert.encoder.layer.5.output.dropout\n",
      " |   (84): bert.encoder.layer.5.output.LayerNorm\n",
      " |   (85): bert.encoder.layer.6\n",
      " |   (86): bert.encoder.layer.6.attention\n",
      " |   (87): bert.encoder.layer.6.attention.self\n",
      " |   (88): bert.encoder.layer.6.attention.self.query\n",
      " |   (89): bert.encoder.layer.6.attention.self.key\n",
      " |   (90): bert.encoder.layer.6.attention.self.value\n",
      " |   (91): bert.encoder.layer.6.attention.self.dropout\n",
      " |   (92): bert.encoder.layer.6.attention.output\n",
      " |   (93): bert.encoder.layer.6.attention.output.dense\n",
      " |   (94): bert.encoder.layer.6.attention.output.dropout\n",
      " |   (95): bert.encoder.layer.6.attention.output.LayerNorm\n",
      " |   (96): bert.encoder.layer.6.intermediate\n",
      " |   (97): bert.encoder.layer.6.intermediate.dense\n",
      " |   (98): bert.encoder.layer.6.output\n",
      " |   (99): bert.encoder.layer.6.output.dense\n",
      " |   (100): bert.encoder.layer.6.output.dropout\n",
      " |   (101): bert.encoder.layer.6.output.LayerNorm\n",
      " |   (102): bert.encoder.layer.7\n",
      " |   (103): bert.encoder.layer.7.attention\n",
      " |   (104): bert.encoder.layer.7.attention.self\n",
      " |   (105): bert.encoder.layer.7.attention.self.query\n",
      " |   (106): bert.encoder.layer.7.attention.self.key\n",
      " |   (107): bert.encoder.layer.7.attention.self.value\n",
      " |   (108): bert.encoder.layer.7.attention.self.dropout\n",
      " |   (109): bert.encoder.layer.7.attention.output\n",
      " |   (110): bert.encoder.layer.7.attention.output.dense\n",
      " |   (111): bert.encoder.layer.7.attention.output.dropout\n",
      " |   (112): bert.encoder.layer.7.attention.output.LayerNorm\n",
      " |   (113): bert.encoder.layer.7.intermediate\n",
      " |   (114): bert.encoder.layer.7.intermediate.dense\n",
      " |   (115): bert.encoder.layer.7.output\n",
      " |   (116): bert.encoder.layer.7.output.dense\n",
      " |   (117): bert.encoder.layer.7.output.dropout\n",
      " |   (118): bert.encoder.layer.7.output.LayerNorm\n",
      " |   (119): bert.encoder.layer.8\n",
      " |   (120): bert.encoder.layer.8.attention\n",
      " |   (121): bert.encoder.layer.8.attention.self\n",
      " |   (122): bert.encoder.layer.8.attention.self.query\n",
      " |   (123): bert.encoder.layer.8.attention.self.key\n",
      " |   (124): bert.encoder.layer.8.attention.self.value\n",
      " |   (125): bert.encoder.layer.8.attention.self.dropout\n",
      " |   (126): bert.encoder.layer.8.attention.output\n",
      " |   (127): bert.encoder.layer.8.attention.output.dense\n",
      " |   (128): bert.encoder.layer.8.attention.output.dropout\n",
      " |   (129): bert.encoder.layer.8.attention.output.LayerNorm\n",
      " |   (130): bert.encoder.layer.8.intermediate\n",
      " |   (131): bert.encoder.layer.8.intermediate.dense\n",
      " |   (132): bert.encoder.layer.8.output\n",
      " |   (133): bert.encoder.layer.8.output.dense\n",
      " |   (134): bert.encoder.layer.8.output.dropout\n",
      " |   (135): bert.encoder.layer.8.output.LayerNorm\n",
      " |   (136): bert.encoder.layer.9\n",
      " |   (137): bert.encoder.layer.9.attention\n",
      " |   (138): bert.encoder.layer.9.attention.self\n",
      " |   (139): bert.encoder.layer.9.attention.self.query\n",
      " |   (140): bert.encoder.layer.9.attention.self.key\n",
      " |   (141): bert.encoder.layer.9.attention.self.value\n",
      " |   (142): bert.encoder.layer.9.attention.self.dropout\n",
      " |   (143): bert.encoder.layer.9.attention.output\n",
      " |   (144): bert.encoder.layer.9.attention.output.dense\n",
      " |   (145): bert.encoder.layer.9.attention.output.dropout\n",
      " |   (146): bert.encoder.layer.9.attention.output.LayerNorm\n",
      " |   (147): bert.encoder.layer.9.intermediate\n",
      " |   (148): bert.encoder.layer.9.intermediate.dense\n",
      " |   (149): bert.encoder.layer.9.output\n",
      " |   (150): bert.encoder.layer.9.output.dense\n",
      " |   (151): bert.encoder.layer.9.output.dropout\n",
      " |   (152): bert.encoder.layer.9.output.LayerNorm\n",
      " |   (153): bert.encoder.layer.10\n",
      " |   (154): bert.encoder.layer.10.attention\n",
      " |   (155): bert.encoder.layer.10.attention.self\n",
      " |   (156): bert.encoder.layer.10.attention.self.query\n",
      " |   (157): bert.encoder.layer.10.attention.self.key\n",
      " |   (158): bert.encoder.layer.10.attention.self.value\n",
      " |   (159): bert.encoder.layer.10.attention.self.dropout\n",
      " |   (160): bert.encoder.layer.10.attention.output\n",
      " |   (161): bert.encoder.layer.10.attention.output.dense\n",
      " |   (162): bert.encoder.layer.10.attention.output.dropout\n",
      " |   (163): bert.encoder.layer.10.attention.output.LayerNorm\n",
      " |   (164): bert.encoder.layer.10.intermediate\n",
      " |   (165): bert.encoder.layer.10.intermediate.dense\n",
      " |   (166): bert.encoder.layer.10.output\n",
      " |   (167): bert.encoder.layer.10.output.dense\n",
      " |   (168): bert.encoder.layer.10.output.dropout\n",
      " |   (169): bert.encoder.layer.10.output.LayerNorm\n",
      " |   (170): bert.encoder.layer.11\n",
      " |   (171): bert.encoder.layer.11.attention\n",
      " |   (172): bert.encoder.layer.11.attention.self\n",
      " |   (173): bert.encoder.layer.11.attention.self.query\n",
      " |   (174): bert.encoder.layer.11.attention.self.key\n",
      " |   (175): bert.encoder.layer.11.attention.self.value\n",
      " |   (176): bert.encoder.layer.11.attention.self.dropout\n",
      " |   (177): bert.encoder.layer.11.attention.output\n",
      " |   (178): bert.encoder.layer.11.attention.output.dense\n",
      " |   (179): bert.encoder.layer.11.attention.output.dropout\n",
      " |   (180): bert.encoder.layer.11.attention.output.LayerNorm\n",
      " |   (181): bert.encoder.layer.11.intermediate\n",
      " |   (182): bert.encoder.layer.11.intermediate.dense\n",
      " |   (183): bert.encoder.layer.11.output\n",
      " |   (184): bert.encoder.layer.11.output.dense\n",
      " |   (185): bert.encoder.layer.11.output.dropout\n",
      " |   (186): bert.encoder.layer.11.output.LayerNorm\n",
      " |   (187): dropout\n",
      " |   (188): classifier\n",
      " |\n",
      "(b): Task name: sentence length (sentlen)\n",
      "(c): Probing dataset(s):\n",
      " | (train):   20 batches of size (at most) 128.\n",
      " | (eval) :   10 batches of size (at most) 256.\n",
      " | (test) :   10 batches of size (at most) 256.\n",
      "(d): Probed module(s) (1 in total):\n",
      " | (0): bert.encoder.layer.0.output.LayerNorm\n",
      "prober_container.probed_modules = ('bert.encoder.layer.0.output.LayerNorm',)\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "# (5): set up a ProbingModelFactory, which combines the probing model and the probing task.\n",
    "probing_factory = curiosidade.ProbingModelFactory(\n",
    "    probing_model_fn=ProbingModel,  # Note: do not instantiate.\n",
    "    optim_fn=functools.partial(torch.optim.Adam, lr=0.001),  # Note: do not instantiate.\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "# (6): attach the probing models to the pretrained model layers.\n",
    "prober_container = curiosidade.attach_probers(\n",
    "    base_model=bert,\n",
    "    probing_model_factory=probing_factory,\n",
    "    modules_to_attach=\"bert.encoder.layer.0.output.LayerNorm\",  # or a container like [\"name_a\", \"name_b\", ...]\n",
    "    random_seed=16,\n",
    "    device=\"cpu\",\n",
    "    prune_unrelated_modules=\"infer\",\n",
    ")\n",
    "\n",
    "print(f\"{prober_container = }\")  # Configuration summary.\n",
    "print(f\"{prober_container.probed_modules = }\")  # Lists all probed module names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "736855d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:57:42.431770Z",
     "start_time": "2022-10-17T13:56:18.347174Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [01:17<00:00, 12.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# (7): train probing models.\n",
    "probing_results = prober_container.train(num_epochs=6, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6824287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:57:42.455565Z",
     "start_time": "2022-10-17T13:57:42.433432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch metric_name                                 module    metric  \\\n",
      "                                                                 mean   \n",
      "0      0    accuracy  bert.encoder.layer.0.output.LayerNorm  0.218359   \n",
      "1      0        loss  bert.encoder.layer.0.output.LayerNorm  1.755534   \n",
      "2      1    accuracy  bert.encoder.layer.0.output.LayerNorm  0.238741   \n",
      "3      1        loss  bert.encoder.layer.0.output.LayerNorm  1.685760   \n",
      "4      2    accuracy  bert.encoder.layer.0.output.LayerNorm  0.308847   \n",
      "5      2        loss  bert.encoder.layer.0.output.LayerNorm  1.622230   \n",
      "6      3    accuracy  bert.encoder.layer.0.output.LayerNorm  0.417050   \n",
      "7      3        loss  bert.encoder.layer.0.output.LayerNorm  1.542018   \n",
      "8      4    accuracy  bert.encoder.layer.0.output.LayerNorm  0.404527   \n",
      "9      4        loss  bert.encoder.layer.0.output.LayerNorm  1.465292   \n",
      "10     5    accuracy  bert.encoder.layer.0.output.LayerNorm  0.500597   \n",
      "11     5        loss  bert.encoder.layer.0.output.LayerNorm  1.363019   \n",
      "\n",
      "              \n",
      "         std  \n",
      "0   0.050343  \n",
      "1   0.060021  \n",
      "2   0.045908  \n",
      "3   0.033664  \n",
      "4   0.066489  \n",
      "5   0.042917  \n",
      "6   0.065261  \n",
      "7   0.037510  \n",
      "8   0.070032  \n",
      "9   0.046813  \n",
      "10  0.058917  \n",
      "11  0.041931  \n"
     ]
    }
   ],
   "source": [
    "# (8): aggregate results.\n",
    "df_train, df_eval, df_test = probing_results.to_pandas(\n",
    "    aggregate_by=[\"batch_index\"],\n",
    "    aggregate_fn=[np.mean, np.std],\n",
    ")\n",
    "\n",
    "print(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4784d4",
   "metadata": {},
   "source": [
    "# Vanilla PyTorch example\n",
    "\n",
    "The single difference between the previous example and this example resides in how the `fn_text_to_tensor_for_huggingface_transformers` is specified. More precisely, HF Transformers requires a batch of training samples in the `HuggingFace datasets` format, whereas vanilla PyTorch models generally uses `torch.data.utils.TensorDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bde99f47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:57:42.462892Z",
     "start_time": "2022-10-17T13:57:42.457486Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import typing as t\n",
    "\n",
    "import curiosidade\n",
    "import torch\n",
    "import torch.nn\n",
    "import tokenizers\n",
    "import buscador\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_name = \"512_hidden_dim_6000_vocab_size_1_layer_lstm\"\n",
    "tokenizer_name = \"6000_subword_tokenizer\"\n",
    "\n",
    "buscador.download_resource(\n",
    "    task_name=\"legal_text_segmentation\",\n",
    "    resource_name=model_name,\n",
    ")\n",
    "\n",
    "buscador.download_resource(\n",
    "    task_name=\"legal_text_segmentation\",\n",
    "    resource_name=tokenizer_name,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d137d46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:57:42.542083Z",
     "start_time": "2022-10-17T13:57:42.465291Z"
    }
   },
   "outputs": [],
   "source": [
    "# (2): load your pretrained model.\n",
    "class LSTMSegmenter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=6000,\n",
    "            embedding_dim=768,\n",
    "            padding_idx=0,\n",
    "        )\n",
    "\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=768,\n",
    "            hidden_size=512,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "\n",
    "        self.lin_out = torch.nn.Linear(2 * 512, 4)\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor):\n",
    "        out = input_ids\n",
    "\n",
    "        out = self.embeddings(out)\n",
    "        out, *_ = self.lstm(out)\n",
    "        out = self.lin_out(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "pretrained_state_dict = torch.load(f\"{model_name}.pt\")\n",
    "lstm = LSTMSegmenter()\n",
    "lstm.load_state_dict(pretrained_state_dict)\n",
    "tokenizer = tokenizers.Tokenizer.from_file(os.path.join(tokenizer_name, \"tokenizer.json\"))\n",
    "\n",
    "# (3): set up your probing model.\n",
    "ProbingModel = curiosidade.probers.utils.get_probing_model_for_sequences(hidden_layer_dims=[128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4df2d349",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:57:43.048321Z",
     "start_time": "2022-10-17T13:57:42.543934Z"
    }
   },
   "outputs": [],
   "source": [
    "# (4): set up a preconfigured probing task.\n",
    "def fn_text_to_tensor_for_pytorch(\n",
    "    content: list[str],\n",
    "    labels: list[int],\n",
    "    split: t.Literal[\"train\", \"eval\", \"test\"],\n",
    ") -> dict[str, torch.Tensor]:\n",
    "    n = 2500 # Subsampling to speed up this example.\n",
    "    content = content[:n]\n",
    "    labels = labels[:n]\n",
    "    \n",
    "    X = torch.nn.utils.rnn.pad_sequence([\n",
    "        torch.Tensor(inst.ids)[:48]\n",
    "        for inst in tokenizer.encode_batch(content)\n",
    "    ], batch_first=True, padding_value=0.0)\n",
    "    \n",
    "    y = torch.Tensor(labels)\n",
    "    \n",
    "    X = X.long()\n",
    "    y = y.long()\n",
    "\n",
    "    return torch.utils.data.TensorDataset(X, y)\n",
    "\n",
    "\n",
    "def accuracy_fn(logits, target):\n",
    "    _, cls_ids = logits.max(axis=-1)\n",
    "    return {\"accuracy\": (cls_ids == target).float().mean().item()}\n",
    "\n",
    "\n",
    "task = curiosidade.ProbingTaskSentenceLength(\n",
    "    fn_text_to_tensor=fn_text_to_tensor_for_pytorch,\n",
    "    metrics_fn=accuracy_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f15f11a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:57:53.468740Z",
     "start_time": "2022-10-17T13:57:43.050072Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prober_container = ProbingModelContainer:\n",
      "(a): Base model: InferencePrunerExtensor(TorchModuleAdapter(LSTMSegmenter(\n",
      " |  |  (embeddings): Embedding(6000, 768, padding_idx=0)\n",
      " |  |  (lstm): LSTM(768, 512, batch_first=True, bidirectional=True)\n",
      " |  |  (lin_out): Linear(in_features=1024, out_features=4, bias=True)\n",
      " |  |)))\n",
      " | (a): No pruned modules.\n",
      " |\n",
      "(b): Task name: sentence length (sentlen)\n",
      "(c): Probing dataset(s):\n",
      " | (train):   20 batches of size (at most) 128.\n",
      " | (eval) :   10 batches of size (at most) 256.\n",
      " | (test) :   10 batches of size (at most) 256.\n",
      "(d): Probed module(s) (1 in total):\n",
      " | (0): lstm\n",
      "prober_container.probed_modules = ('lstm',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:07<00:00,  1.30s/it]\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "\n",
    "# (5): set up a ProbingModelFactory, which combines the probing model and the probing task.\n",
    "probing_factory = curiosidade.ProbingModelFactory(\n",
    "    probing_model_fn=ProbingModel,  # Note: do not instantiate.\n",
    "    optim_fn=functools.partial(torch.optim.Adam, lr=0.001),  # Note: do not instantiate.\n",
    "    task=task,\n",
    ")\n",
    "\n",
    "# (6): attach the probing models to the pretrained model layers.\n",
    "prober_container = curiosidade.attach_probers(\n",
    "    base_model=lstm,\n",
    "    probing_model_factory=probing_factory,\n",
    "    modules_to_attach=\"lstm\",  # or a container like [\"name_a\", \"name_b\", ...]\n",
    "    random_seed=16,\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "print(f\"{prober_container = }\")  # Configuration summary.\n",
    "print(f\"{prober_container.probed_modules = }\")  # Lists all probed module names.\n",
    "\n",
    "# (7): train probing models.\n",
    "probing_results = prober_container.train(num_epochs=6, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fd6d128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-17T13:57:53.487959Z",
     "start_time": "2022-10-17T13:57:53.470076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   epoch metric_name module    metric          \n",
      "                                 mean       std\n",
      "0      0    accuracy   lstm  0.226723  0.059499\n",
      "1      0        loss   lstm  1.725622  0.052323\n",
      "2      1    accuracy   lstm  0.337569  0.054889\n",
      "3      1        loss   lstm  1.619361  0.041703\n",
      "4      2    accuracy   lstm  0.395634  0.065277\n",
      "5      2        loss   lstm  1.495245  0.045395\n",
      "6      3    accuracy   lstm  0.446415  0.056187\n",
      "7      3        loss   lstm  1.373625  0.047485\n",
      "8      4    accuracy   lstm  0.456250  0.055614\n",
      "9      4        loss   lstm  1.290307  0.054287\n",
      "10     5    accuracy   lstm  0.497243  0.061581\n",
      "11     5        loss   lstm  1.211972  0.047279\n"
     ]
    }
   ],
   "source": [
    "# (8): aggregate results.\n",
    "df_train, df_eval, df_test = probing_results.to_pandas(\n",
    "    aggregate_by=[\"batch_index\"],\n",
    "    aggregate_fn=[np.mean, np.std],\n",
    ")\n",
    "\n",
    "print(df_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
